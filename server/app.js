// ============= server/app.js (CORSä¿®å¤ç‰ˆ, ä»£ç†é€‰æ‹©æ€§åº”ç”¨) =============
const express = require("express");
const cors = require("cors");
const axios = require("axios");
const { HttpsProxyAgent } = require("https-proxy-agent");
require("dotenv").config();

const app = express();
const PORT = process.env.PORT || 3001;

// ä¿®å¤CORSé…ç½®
app.use(
  cors({
    origin: [
      "http://localhost:3000",
      "http://127.0.0.1:3000",
      "http://localhost:5173",
      "http://127.0.0.1:5173",
      process.env.FRONTEND_URL,
    ].filter(Boolean),
    credentials: true,
    methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allowedHeaders: ["Content-Type", "Authorization", "X-Requested-With"],
  })
);

// å¤„ç†é¢„æ£€è¯·æ±‚
app.options("*", cors());

app.use(express.json({ limit: "10mb" }));
app.use(express.urlencoded({ extended: true }));

// ä»£ç†é…ç½®
const PROXY_URL = process.env.PROXY_URL;
// å¢å¼ºçš„æ—¥å¿—ä¸­é—´ä»¶
app.use((req, res, next) => {
  const timestamp = new Date().toISOString();
  console.log(
    `[${timestamp}] ${req.method} ${req.path} - Origin: ${
      req.get("Origin") || "none"
    }`
  );
  next();
});

// AIæœåŠ¡é…ç½®
const AI_SERVICES = {
  openai: {
    baseURL: process.env.OPENAI_BASE_URL || "https://api.openai.com/v1",
    defaultModel: "gpt-3.5-turbo",
  },
  anthropic: {
    baseURL: process.env.ANTHROPIC_BASE_URL || "https://api.anthropic.com",
    defaultModel: "claude-3-sonnet",
  },
  zhipu: {
    baseURL:
      process.env.ZHIPU_BASE_URL || "https://open.bigmodel.cn/api/paas/v4",
    defaultModel: "glm-4",
  },
  qwen: {
    baseURL:
      process.env.QWEN_BASE_URL ||
      "https://dashscope.aliyuncs.com/compatible-mode/v1",
    defaultModel: "qwen-turbo",
  },
  gemini: {
    baseURL:
      process.env.GEMINI_BASE_URL ||
      "https://generativelanguage.googleapis.com/v1beta",
    defaultModel: "gemini-pro",
  },
};

/**
 * åˆ›å»ºaxioså®ä¾‹çš„å·¥å‚å‡½æ•°
 * @param {string} provider - æœåŠ¡å•†åç§° (e.g., 'openai', 'zhipu')
 * @param {string} baseURL - APIçš„åŸºç¡€URL
 * @param {object} headers - è‡ªå®šä¹‰è¯·æ±‚å¤´
 * @returns {axios.AxiosInstance}
 */
const createAxiosInstance = (provider, baseURL, headers = {}) => {
  const config = {
    baseURL,
    timeout: 30000,
    headers: {
      "Content-Type": "application/json",
      ...headers,
    },
  };

  // å®šä¹‰å“ªäº›æœåŠ¡éœ€è¦èµ°ä»£ç†
  const providersNeedingProxy = ["openai", "anthropic", "gemini"];

  // å¦‚æœè®¾ç½®äº†ä»£ç†URLï¼Œå¹¶ä¸”å½“å‰æœåŠ¡å•†åœ¨éœ€è¦ä»£ç†çš„åˆ—è¡¨ä¸­ï¼Œåˆ™åº”ç”¨ä»£ç†
  if (PROXY_URL && providersNeedingProxy.includes(provider)) {
    const agent = new HttpsProxyAgent(PROXY_URL);
    config.httpsAgent = agent;
    config.httpAgent = agent;
    console.log(`[${provider}] ä½¿ç”¨ä»£ç†: ${PROXY_URL}`);
  } else {
    console.log(`[${provider}] æœªä½¿ç”¨ä»£ç†`);
  }

  return axios.create(config);
};

// ============= å¥åº·æ£€æŸ¥ (å¢å¼ºç‰ˆ) =============
app.get("/api/health", (req, res) => {
  console.log("æ”¶åˆ°å¥åº·æ£€æŸ¥è¯·æ±‚");

  const healthData = {
    status: "ok",
    timestamp: new Date().toISOString(),
    server: {
      port: PORT,
      nodeVersion: process.version,
      uptime: process.uptime(),
    },
    proxy: PROXY_URL ? "enabled" : "disabled",
    services: Object.keys(AI_SERVICES),
    cors: {
      origin: req.get("Origin") || "none",
      method: req.method,
    },
  };

  if (PROXY_URL) {
    healthData.proxyUrl = PROXY_URL;
    healthData.proxyFor = ["openai", "anthropic", "gemini"];
  }

  console.log("è¿”å›å¥åº·æ£€æŸ¥æ•°æ®:", healthData);
  res.json(healthData);
});

// ç®€å•çš„æ ¹è·¯å¾„å“åº”
app.get("/", (req, res) => {
  res.json({
    message: "AI Chat Backend Server",
    version: "1.0.0",
    endpoints: [
      "/api/health",
      "/api/openai/chat",
      "/api/zhipu/chat",
      "/api/anthropic/chat",
      "/api/qwen/chat",
      "/api/gemini/chat",
      "/api/test",
    ],
  });
});

// ============= OpenAI APIå¤„ç† =============
app.post("/api/openai/chat", async (req, res) => {
  console.log("æ”¶åˆ°OpenAIèŠå¤©è¯·æ±‚");

  try {
    const { messages, model, stream = true, apiKey, ...otherParams } = req.body;

    if (!apiKey) {
      console.log("APIå¯†é’¥ç¼ºå¤±");
      return res.status(400).json({ error: "APIå¯†é’¥æœªæä¾›" });
    }

    // === MODIFICATION START: ä¼ å…¥æœåŠ¡å•†åç§° ===
    const axiosInstance = createAxiosInstance(
      "openai",
      AI_SERVICES.openai.baseURL,
      {
        Authorization: `Bearer ${apiKey}`,
      }
    );
    // === MODIFICATION END ===

    const payload = {
      model: model || AI_SERVICES.openai.defaultModel,
      messages,
      stream,
      temperature: 0.7,
      max_tokens: 2000,
      ...otherParams,
    };

    console.log(
      `å‘é€è¯·æ±‚åˆ°OpenAI: ${AI_SERVICES.openai.baseURL}/chat/completions`
    );

    if (stream) {
      const response = await axiosInstance.post("/chat/completions", payload, {
        responseType: "stream",
      });

      res.setHeader("Content-Type", "text/event-stream");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");
      res.setHeader("Access-Control-Allow-Origin", "*");

      response.data.on("data", (chunk) => {
        res.write(chunk);
      });

      response.data.on("end", () => {
        console.log("OpenAIæµå¼å“åº”å®Œæˆ");
        res.end();
      });

      response.data.on("error", (error) => {
        console.error("OpenAIæµå¼å“åº”é”™è¯¯:", error);
        if (!res.headersSent) {
          res.status(500).json({ error: "æµå¼å“åº”é”™è¯¯" });
        }
      });
    } else {
      const response = await axiosInstance.post("/chat/completions", payload);
      console.log("OpenAIéæµå¼å“åº”æˆåŠŸ");
      res.json(response.data);
    }
  } catch (error) {
    console.error("OpenAI APIé”™è¯¯:", error.response?.data || error.message);
    if (!res.headersSent) {
      res.status(error.response?.status || 500).json({
        error:
          error.response?.data?.error?.message ||
          error.message ||
          "OpenAI APIè°ƒç”¨å¤±è´¥",
      });
    }
  }
});

// ============= æ™ºè°±AIå¤„ç† =============
app.post("/api/zhipu/chat", async (req, res) => {
  console.log("æ”¶åˆ°æ™ºè°±AIèŠå¤©è¯·æ±‚");

  try {
    const { messages, model, stream = true, apiKey, ...otherParams } = req.body;

    if (!apiKey) {
      return res.status(400).json({ error: "APIå¯†é’¥æœªæä¾›" });
    }

    // === MODIFICATION START: ä¼ å…¥æœåŠ¡å•†åç§° ===
    const axiosInstance = createAxiosInstance(
      "zhipu",
      AI_SERVICES.zhipu.baseURL,
      {
        Authorization: `Bearer ${apiKey}`,
      }
    );
    // === MODIFICATION END ===

    const payload = {
      model: model || AI_SERVICES.zhipu.defaultModel,
      messages,
      stream,
      temperature: 0.7,
      max_tokens: 2000,
      ...otherParams,
    };

    if (stream) {
      const response = await axiosInstance.post("/chat/completions", payload, {
        responseType: "stream",
      });

      res.setHeader("Content-Type", "text/event-stream");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");
      res.setHeader("Access-Control-Allow-Origin", "*");

      response.data.pipe(res);
    } else {
      const response = await axiosInstance.post("/chat/completions", payload);
      res.json(response.data);
    }
  } catch (error) {
    console.error("æ™ºè°±AIé”™è¯¯:", error.response?.data || error.message);
    if (!res.headersSent) {
      res.status(error.response?.status || 500).json({
        error:
          error.response?.data?.error?.message ||
          error.message ||
          "æ™ºè°±AIè°ƒç”¨å¤±è´¥",
      });
    }
  }
});

// ============= Anthropic Claudeå¤„ç† =============
app.post("/api/anthropic/chat", async (req, res) => {
  try {
    const { messages, model, stream = true, apiKey, ...otherParams } = req.body;

    if (!apiKey) {
      return res.status(400).json({ error: "APIå¯†é’¥æœªæä¾›" });
    }

    // === MODIFICATION START: ä¼ å…¥æœåŠ¡å•†åç§° ===
    const axiosInstance = createAxiosInstance(
      "anthropic",
      AI_SERVICES.anthropic.baseURL,
      {
        "x-api-key": apiKey,
        "anthropic-version": "2023-06-01",
      }
    );
    // === MODIFICATION END ===

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const systemMessage = messages.find((m) => m.role === "system");
    const chatMessages = messages.filter((m) => m.role !== "system");

    const payload = {
      model: model || AI_SERVICES.anthropic.defaultModel,
      max_tokens: 2000,
      messages: chatMessages,
      stream,
      ...otherParams,
    };

    if (systemMessage) {
      payload.system = systemMessage.content;
    }

    if (stream) {
      const response = await axiosInstance.post("/v1/messages", payload, {
        responseType: "stream",
      });

      res.setHeader("Content-Type", "text/event-stream");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");

      response.data.pipe(res);
    } else {
      const response = await axiosInstance.post("/v1/messages", payload);
      res.json(response.data);
    }
  } catch (error) {
    console.error("Claude APIé”™è¯¯:", error.response?.data || error.message);
    res.status(error.response?.status || 500).json({
      error:
        error.response?.data?.error?.message ||
        error.message ||
        "Claude APIè°ƒç”¨å¤±è´¥",
    });
  }
});

// ============= é€šä¹‰åƒé—®å¤„ç† =============
app.post("/api/qwen/chat", async (req, res) => {
  try {
    const { messages, model, stream = true, apiKey, ...otherParams } = req.body;
    if (!apiKey) {
      return res.status(400).json({ error: "APIå¯†é’¥æœªæä¾›" });
    }
    console.log("æ”¶åˆ°é€šä¹‰åƒé—®AIèŠå¤©è¯·æ±‚", AI_SERVICES.qwen.baseURL);
    const axiosInstance = createAxiosInstance(
      "qwen",
      AI_SERVICES.qwen.baseURL,
      {
        Authorization: `Bearer ${apiKey}`,
        ...(stream && { Accept: "text/event-stream" }),
      }
    );

    const payload = {
      model: model || AI_SERVICES.qwen.defaultModel,
      messages,
      temperature: otherParams.temperature ?? 0.7,
      stream,
      ...otherParams,
    };

    // å…³é”®ä¿®æ­£ #2: ä½¿ç”¨å…¼å®¹æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ /chat/completions
    const endpoint = "/chat/completions";
    console.log("stream:", stream);

    if (stream) {
      const upstream = await axiosInstance.post(endpoint, payload, {
        responseType: "stream",
      });
      res.setHeader("Content-Type", "text/event-stream");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");
      console.log("é€šä¹‰åƒé—®AIæµå¼å“åº”å¼€å§‹", res);
      upstream.data.pipe(res);
    } else {
      const { data } = await axiosInstance.post(endpoint, payload);
      res.json(data);
    }
  } catch (err) {
    console.error("[qwen] è°ƒç”¨å¤±è´¥:", err.response?.data || err.message);
    const status = err.response?.status || 500;
    const msg =
      err.response?.data?.error?.message || // å…¼å®¹æ¨¡å¼ä¼šè¿”å› error.message
      err.response?.data?.message || // åŸç”Ÿæ¨¡å¼å¯èƒ½è¿”å› message
      err.message ||
      "é€šä¹‰åƒé—®è°ƒç”¨å¤±è´¥";
    if (!res.headersSent) res.status(status).json({ error: msg });
  }
});

// ============= Google Geminiå¤„ç† =============
app.post("/api/gemini/chat", async (req, res) => {
  try {
    const { messages, model, stream = true, apiKey, ...otherParams } = req.body;

    if (!apiKey) {
      return res.status(400).json({ error: "APIå¯†é’¥æœªæä¾›" });
    }

    const modelName = model || AI_SERVICES.gemini.defaultModel;
    const endpoint = stream
      ? `/models/${modelName}:streamGenerateContent?key=${apiKey}`
      : `/models/${modelName}:generateContent?key=${apiKey}`;

    // === MODIFICATION START: ä¼ å…¥æœåŠ¡å•†åç§° ===
    const axiosInstance = createAxiosInstance(
      "gemini",
      AI_SERVICES.gemini.baseURL
    );
    // === MODIFICATION END ===

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const systemMessage = messages.find((m) => m.role === "system");
    const chatMessages = messages.filter((m) => m.role !== "system");

    const payload = {
      contents: chatMessages.map((msg) => ({
        role: msg.role === "assistant" ? "model" : "user",
        parts: [{ text: msg.content }],
      })),
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 2000,
        ...otherParams,
      },
    };

    if (systemMessage) {
      payload.systemInstruction = {
        parts: [{ text: systemMessage.content }],
      };
    }

    if (stream) {
      const response = await axiosInstance.post(endpoint, payload, {
        responseType: "stream",
      });

      res.setHeader("Content-Type", "text/event-stream");
      res.setHeader("Cache-Control", "no-cache");
      res.setHeader("Connection", "keep-alive");

      response.data.pipe(res);
    } else {
      const response = await axiosInstance.post(endpoint, payload);
      res.json(response.data);
    }
  } catch (error) {
    console.error("Gemini APIé”™è¯¯:", error.response?.data || error.message);
    res.status(error.response?.status || 500).json({
      error:
        error.response?.data?.error?.message ||
        error.message ||
        "Gemini APIè°ƒç”¨å¤±è´¥",
    });
  }
});

// ============= é…ç½®æµ‹è¯• (å¢å¼ºç‰ˆ) =============
app.post("/api/test", async (req, res) => {
  console.log("æ”¶åˆ°APIæµ‹è¯•è¯·æ±‚");
  try {
    const { provider, apiKey } = req.body;
    if (!provider || !apiKey) {
      return res.status(400).json({ error: "ç¼ºå°‘æœåŠ¡å•†æˆ–API Key" });
    }
    console.log(`æµ‹è¯• ${provider} è¿æ¥...`);

    let testResult = false;
    let errorMessage = "";
    let axiosInstance;

    switch (provider) {
      case "openai":
        axiosInstance = createAxiosInstance(
          "openai",
          AI_SERVICES.openai.baseURL,
          {
            Authorization: `Bearer ${apiKey}`,
          }
        );
        try {
          await axiosInstance.get("/models");
          testResult = true;
        } catch (e) {
          errorMessage = e.response?.data?.error?.message || e.message;
        }
        break;

      case "zhipu":
        axiosInstance = createAxiosInstance(
          "zhipu",
          AI_SERVICES.zhipu.baseURL,
          {
            Authorization: `Bearer ${apiKey}`,
          }
        );
        try {
          await axiosInstance.post("/chat/completions", {
            model: "glm-3-turbo",
            messages: [{ role: "user", content: "Hi" }],
            max_tokens: 1,
          });
          testResult = true;
        } catch (e) {
          errorMessage = e.response?.data?.error?.message || e.message;
        }
        break;

      case "anthropic":
        axiosInstance = createAxiosInstance(
          "anthropic",
          AI_SERVICES.anthropic.baseURL,
          {
            "x-api-key": apiKey,
            "anthropic-version": "2023-06-01",
          }
        );
        try {
          await axiosInstance.post("/messages", {
            model: AI_SERVICES.anthropic.defaultModel,
            messages: [{ role: "user", content: "Hi" }],
            max_tokens: 10,
          });
          testResult = true;
        } catch (e) {
          errorMessage = e.response?.data?.error?.message || e.message;
        }
        break;

      case "qwen":
        axiosInstance = createAxiosInstance("qwen", AI_SERVICES.qwen.baseURL, {
          Authorization: `Bearer ${apiKey}`,
        });
        try {
          await axiosInstance.post("/chat/completions", {
            model: AI_SERVICES.qwen.defaultModel,
            messages: [{ role: "user", content: "Hi" }],
            max_tokens: 2,
          });
          testResult = true;
        } catch (e) {
          errorMessage = e.response?.data?.error?.message || e.message;
        }
        break;

      case "gemini":
        try {
          const endpoint = `/models/gemini-pro:generateContent?key=${apiKey}`;
          axiosInstance = createAxiosInstance(
            "gemini",
            AI_SERVICES.gemini.baseURL
          );
          await axiosInstance.post(endpoint, {
            contents: [{ parts: [{ text: "Hi" }] }],
          });
          testResult = true;
        } catch (e) {
          errorMessage = e.response?.data?.error?.message || e.message;
        }
        break;

      default:
        errorMessage = "ä¸æ”¯æŒçš„æœåŠ¡å•†";
    }

    if (testResult) {
      res.json({ success: true, message: "APIè¿æ¥æµ‹è¯•æˆåŠŸ" });
    } else {
      res
        .status(400)
        .json({ success: false, message: `è¿æ¥å¤±è´¥: ${errorMessage}` });
    }
  } catch (error) {
    res.status(500).json({ success: false, message: error.message });
  }
});

// å…¨å±€é”™è¯¯å¤„ç†ä¸­é—´ä»¶
app.use((error, req, res, next) => {
  // <-- Correct: 4 arguments now
  console.error("æœåŠ¡å™¨é”™è¯¯:", error);
  if (!res.headersSent) {
    res.status(500).json({ error: "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", details: error.message });
  }
});
// 404å¤„ç†
app.use("*", (req, res) => {
  console.log(`404 - æœªæ‰¾åˆ°è·¯å¾„: ${req.method} ${req.originalUrl}`);
  res.status(404).json({
    error: "æ¥å£ä¸å­˜åœ¨",
    path: req.originalUrl,
    method: req.method,
    availableEndpoints: [
      "/api/health",
      "/api/openai/chat",
      "/api/zhipu/chat",
      "/api/anthropic/chat",
      "/api/qwen/chat",
      "/api/gemini/chat",
      "/api/test",
    ],
  });
});

// å¯åŠ¨æœåŠ¡å™¨
const server = app.listen(PORT, "0.0.0.0", () => {
  console.log("=================================");
  console.log(`ğŸš€ åç«¯æœåŠ¡å™¨è¿è¡ŒæˆåŠŸï¼`);
  console.log(`ğŸ“¡ æœåŠ¡åœ°å€: http://localhost:${PORT}`);
  console.log(
    `ğŸ“¡ å‰ç«¯åœ°å€: ${process.env.FRONTEND_URL || "http://localhost:3000"}`
  );
  // === MODIFICATION START: æ›´æ–°ä»£ç†çŠ¶æ€æ—¥å¿— ===
  console.log(
    `ğŸŒ ä»£ç†é…ç½®: ${
      PROXY_URL
        ? `å·²å¯ç”¨ (${PROXY_URL})ï¼Œå°†ç”¨äº openai, anthropic, gemini`
        : "æœªå¯ç”¨"
    }`
  );
  // === MODIFICATION END ===
  console.log(`ğŸ”— å¥åº·æ£€æŸ¥: http://localhost:${PORT}/api/health`);
  console.log(`ğŸ“‹ å¯ç”¨æœåŠ¡: ${Object.keys(AI_SERVICES).join(", ")}`);
  console.log("=================================");
});

// ä¼˜é›…å…³é—­
process.on("SIGTERM", () => {
  console.log("æ”¶åˆ°SIGTERMä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­æœåŠ¡å™¨...");
  server.close(() => {
    console.log("æœåŠ¡å™¨å·²å…³é—­");
    process.exit(0);
  });
});

module.exports = app;
